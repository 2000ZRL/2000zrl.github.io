<meta name="description" content="Ronglai Zuo (左镕来)">
<!-- <meta name="viewport" content="width=device-width, initial-scale=1"> -->
<link rel="stylesheet" href="./files/jemdoc.css" type="text/css;charset=utf-8">
<link rel="shortcut icon" href="./files/ten.ico"/>
<title>Ronglai Zuo (左镕来)</title>


<body>

<div id="layout-content" style="margin-top:25px">

<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">					
					<h1>Ronglai Zuo (左镕来)</h1></div>

				<h3>Ph.D. Candidate</h3>  
				<p>
					Room 4209, Academic Building <br>
					HKUST, Clear Water Bay <br>
					Kowloon, Hong Kong <br>
					<br>
Email:  <a href="mailto:rzuo@cse.ust.hk">rzuo [at] cse.ust.hk</a>; <a href="mailto:zrl2016ustc@outlook.com">zrl2016ustc [at] outlook.com</a> <br>
<!-- Github: <a href="https://github.com/2000ZRL">2000ZRL</a> <br> -->
<br>
<a href="./files/Ronglai_CV.pdf">[CV]</a> <a href="https://scholar.google.com/citations?user=vyCvXx8AAAAJ&hl=zh-CN">[Google Scholar]</a> <a href="https://www.linkedin.com/in/%E9%95%95%E6%9D%A5-%E5%B7%A6-298108180/?locale=en_US">[Linkedin]</a> <!--<a href="https://twitter.com/TianyuPang1">[Twitter]</a> -->
					<br>
				</p>
			</td>
			<td>
				<img src="./files/ronglai_1.jpg" border="0" width="200">
			</td>
		</tr><tr>
	</tr></tbody>
</table>


<h2>Biography</h2>
    <p>I am currently a final-year Ph.D. candidate at Department of Computer Science and Engineering, The Hong Kong University of Science and Technology (HKUST), supervised by <a href="http://home.cse.ust.hk/~mak/profile.html">Prof. Brian Mak</a>. Before that, I received my B.Eng. degree of Electronic Information Engineering from Special Class for the Gifted Young, University of Science and Technology of China (USTC), in 2020. I am now a research intern at Microsoft Research Asia supervised by <a href="https://scholar.google.com/citations?user=-ncz2s8AAAAJ&hl=zh-CN&oi=sra">Fangyun Wei</a>.</p>
        
    <p>My research mainly focuses on sign language understanding and generation, while I am also interested in video understanding and multimodal learning. </p>

    <p><font color="#FF0000">Looking for a postdoctoral position starting in Fall 2024! Please drop me an email if you are interested in my background.</font></p>


<h2>News</h2>
<ul>
<div style="height:200px;width:device-width;overflow:auto;background:#FFFFFF;">
    <li>
        <p>[01/2024] One paper is accepted by ACM TOMM.</p>
    </li>
    <li>
        <p>[02/2023] One paper is accepted by CVPR 2023.</p>
    </li>
    <li>
        <p>[09/2022] One paper is accepted by NeurIPS 2022.</p>
    </li>
    <li>
        <p>[06/2022] One paper is accepted by Interspeech 2022.</p>
    </li>
    <li>
        <p>[04/2022] Start my internship at MSRA!</p>
    </li>
    <li>
        <p>[03/2022] One paper is accepted by CVPR 2022.</p>
    </li>
    <li>
        <p>[12/2021] Pass my Ph.D. qualifying exam. Now I am a Ph.D. candidate!</p>
    </li>
    <li>
        <p>[09/2020] Start my Ph.D. journey at HKUST!</p>
    <li>
        <p>[07/2020] Finish my undergraduate study at USTC. Memorable 4 years in Hefei!</p>
    </li>
</div>
</ul>


<h2>Publications</h2> (*indicates co-first authors)
<ul>
	<li>
        <a href='https://arxiv.org/abs/2212.13023v2'>Improving Continuous Sign Language Recognition with Consistency Constraints and Signer Removal</a> <br>
        <strong><u>Ronglai Zuo</u></strong> and Brian Mak<br>
        ACM Transactions on Multimedia Computing, Communications, and Applications <strong>(TOMM)</strong>, accepted, 2024<br>
        <!-- <a href="https://github.com/ShawnXYang/AccumulativeAttack">[code]</a> -->
        <a href="https://arxiv.org/pdf/2212.13023v2.pdf">[pdf]</a>
	<a href="https://github.com/2000ZRL/LCSA_C2SLR_SRM">[code]</a>
   </li>
	<li>
        <a href='https://openaccess.thecvf.com/content/CVPR2023/html/Zuo_Natural_Language-Assisted_Sign_Language_Recognition_CVPR_2023_paper.html'>Natural Language-Assisted Sign Language Recognition</a> <br>
        <strong><u>Ronglai Zuo</u></strong>, Fangyun Wei, and Brian Mak<br>
        IEEE/CVF Conference on Computer Vision and Pattern Recognition <strong>(CVPR)</strong>, Vancouver, Canada, 2023 <br>
        <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zuo_Natural_Language-Assisted_Sign_Language_Recognition_CVPR_2023_paper.pdf">[pdf]</a>
	<a href="https://github.com/FangyunWei/SLRT">[code]</a>
   </li>
	<li>
        <a href='https://papers.nips.cc/paper_files/paper/2022/hash/6cd3ac24cdb789beeaa9f7145670fcae-Abstract-Conference.html'>Two-Stream Network for Sign Language Recognition and Translation</a> <br>
        Yutong Chen*, <strong><u>Ronglai Zuo</u>*</strong>, Fangyun Wei*, Yu Wu, Shujie Liu, and Brian Mak<br>
        Advances in Neural Information Processing Systems <strong>(NeurIPS)</strong>, New Orleans, USA, 2022, <strong><i>Spotlight</i></strong> <br>
        <a href="https://papers.nips.cc/paper_files/paper/2022/file/6cd3ac24cdb789beeaa9f7145670fcae-Paper-Conference.pdf">[pdf]</a>
	<a href="https://github.com/FangyunWei/SLRT">[code]</a>
    </li>
	<li>
        <a href='https://openaccess.thecvf.com/content/CVPR2022/html/Zuo_C2SLR_Consistency-Enhanced_Continuous_Sign_Language_Recognition_CVPR_2022_paper.html'>C2SLR: Consistency-enhanced Continuous Sign Language Recognition</a> <br>
        <strong><u>Ronglai Zuo</u></strong> and Brian Mak<br>
        IEEE/CVF Conference on Computer Vision and Pattern Recognition <strong>(CVPR)</strong>, New Orleans, USA, 2022 <br>
        <!-- <a href="https://github.com/ShawnXYang/AccumulativeAttack">[code]</a> -->
        <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zuo_C2SLR_Consistency-Enhanced_Continuous_Sign_Language_Recognition_CVPR_2022_paper.pdf">[pdf]</a>
	<a href="https://github.com/2000ZRL/LCSA_C2SLR_SRM">[code]</a>
    </li>
    <li>
        <a href='https://www.isca-speech.org/archive/interspeech_2022/zuo22_interspeech.html'>Local Context-aware Self-attention for Continuous Sign Language Recognition</a> <br>
        <strong><u>Ronglai Zuo</u></strong> and Brian Mak <br>
        Annual Conference of International Speech Communication Association <strong>(Interspeech)</strong>, Incheon, Korea, 2022 <br>
        <!-- <a href="https://github.com/ShawnXYang/AccumulativeAttack">[code]</a> -->
        <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2022/zuo22_interspeech.pdf">[pdf]</a>
	<a href="https://github.com/2000ZRL/LCSA_C2SLR_SRM">[code]</a>
    </li>
</ul>

<h3 style="color:black; font-weight:normal">Preprints</h3>
<ul>
   <li>
        <a href='https://arxiv.org/abs/2401.05336'>Towards Online Sign Language Recognition and Translation</a><br>
        <strong><u>Ronglai Zuo</u></strong>, Fangyun Wei, and Brian Mak<br>
        Under Review, 2023<br>
        <a href="https://arxiv.org/pdf/2401.05336.pdf">[pdf]</a>
	<a href="https://github.com/FangyunWei/SLRT">[code]</a>
   </li>
   <li>
        <a href='https://arxiv.org/abs/2401.04730'>A Simple Baseline for Spoken Language to Sign Language Translation with 3D Avatars</a> <br>
        <strong><u>Ronglai Zuo</u>*</strong>, Fangyun Wei*, Zenggui Chen, Brian Mak, Jiaolong Yang, and Xin Tong<br>
        Under Review, 2023<br>
        <a href="https://arxiv.org/pdf/2401.04730.pdf">[pdf]</a>
	<a href="https://github.com/FangyunWei/SLRT">[code]</a>
	<a href="https://2000zrl.github.io/spoken2sign/">[project]</a>
   </li>
   <li>
        <a href='https://2000zrl.github.io'>A Hong Kong Sign Language Corpus Collected from Sign-interpreted TV News</a> <br>
        Zhe Niu*, <strong><u>Ronglai Zuo</u>*</strong>, Brian Mak, and Fangyun Wei<br>
        Under Review, 2023<br>
        [pdf] [dataset]<br>
   </li>
</ul>



<!-- <h2>Preprints and under review</h2>
<ul>
    <li>
         <a href='https://arxiv.org/abs/2105.14785'>Adversarial Training with Rectified Rejection</a> <br>
         <strong><u>Tianyu Pang</u></strong>, Huishuai Zhang, Di He, Yinpeng Dong, Hang Su, Wei Chen, Jun Zhu, Tie-Yan Liu<br>
         arXiv preprint 2105.14785<br>
         <a href="https://github.com/P2333/Rectified-Rejection">[code]</a>
         <a href="https://arxiv.org/abs/2105.14785">[appendix]</a>
    </li>
    <li>
         <a href='https://arxiv.org/abs/2107.01809'>Boosting Transferability of Targeted Adversarial Examples via Hierarchical Generative Networks</a> <br>
         Xiao Yang, Yinpeng Dong, <strong><u>Tianyu Pang</u></strong>, Hang Su, Jun Zhu<br>
         arXiv preprint 2107.01809<br>
    </li>       
</ul> -->


<h2>Honors &amp; Awards</h2>
<ul>
    <li>
        <strong>Stars of Tomorrow</strong>, Microsoft Research Asia, 2023
    </li>
    <li>
        <strong>Outstanding Graduate</strong>, USTC, 2020
    </li>
    <li>
        <strong>Bronze Award for Outstanding Students</strong>, USTC, 2017-2019
    </li>
</ul>


<h2>Services</h2>
<ul>
    <li>
        Conference Reviewer: <strong>CVPR</strong>, <strong>ICCV</strong>, <strong>NeurIPS</strong>, <strong>ICLR</strong>
    </li>
    <li>
        Journal Reviewer: <strong>TMM</strong>, <strong>PR</strong>, <strong>IPM</strong>
    </li>
</ul>


<h2>Teaching</h2>
<ul>
    <li>
        TA in <strong>COMP2012 Object-Oriented Programming and Data Structures</strong>, Fall 2023
    </li>
    <li>
        TA in <strong>COMP2011 Programming with C++</strong>, Spring 2021, Fall 2021
    </li>
</ul>


<div id="footer">
	<div id="footer-text"></div>
</div>
&copy 2022-2024 Ronglai Zuo. Last updated in 01/2024.
</body></html>
