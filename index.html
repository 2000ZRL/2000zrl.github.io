<meta name="description" content="Ronglai Zuo (左镕来)">
<link rel="stylesheet" href="./files/jemdoc.css" type="text/css;charset=utf-8">
<title>Ronglai Zuo (左镕来)</title>


<body>

<div id="layout-content" style="margin-top:25px">

<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">					
					<h1>Ronglai Zuo (左镕来)</h1></div>

				<h3>Ph.D. Candidate</h3>  
				<p>
					Room 4209, Academic Building <br>
					HKUST, Clear Water Bay <br>
					New Territory, Hong Kong <br>
					<br>
Email:  <a href="mailto:rzuo@cse.ust.hk">rzuo [at] cse.ust.hk</a>; <a href="mailto:zrl2016ustc@outlook.com">zrl2016ustc [at] outlook.com</a> <br>
<!-- Github: <a href="https://github.com/2000ZRL">2000ZRL</a> <br> -->
<br>
<a href="./files/Ronglai_CV.pdf">[CV]</a> <a href="https://scholar.google.com/citations?user=vyCvXx8AAAAJ&hl=zh-CN">[Google Scholar]</a> <a href="https://www.linkedin.com/in/%E9%95%95%E6%9D%A5-%E5%B7%A6-298108180/?locale=en_US">[Linkedin]</a> <!--<a href="https://twitter.com/TianyuPang1">[Twitter]</a> -->
					<br>
				</p>
			</td>
			<td>
				<img src="./files/ronglai_1.jpg" border="0" width="200">
			</td>
		</tr><tr>
	</tr></tbody>
</table>


<h2>Biography</h2>
    <p>I am currently a Ph.D. candidate at Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, supervised by <a href="http://home.cse.ust.hk/~mak/profile.html">Prof. Brian Mak</a>. Before that, I received my B.Eng. degree of Electronic Information Engineering from Special Class for the Gifted Young, University of Science and Technology of China, in 2020. I am now a research intern at Microsoft Research Asia supervised by <a href="https://scholar.google.com/citations?user=-ncz2s8AAAAJ&hl=zh-CN&oi=sra">Fangyun Wei</a>.</p>
        
    <p>My current research mainly focus on sign language understanding (recognition and translation), while I am also interested in video understanding and multi-modality learning. </p>


<h2>News</h2>
<ul>
<div style="height:200px;width:device-width;overflow:auto;background:#FFFFFF;">
    <li>
        <p>[09/2022] One paper is accepted by NeurIPS 2022.</p>
    </li>
    <li>
        <p>[06/2022] One paper is accepted by InterSpeech 2022.</p>
    </li>
    <li>
        <p>[04/2022] Start my internship at MSRA!</p>
    </li>
    <li>
        <p>[03/2022] One paper is accepted by CVPR 2022.</p>
    </li>
    <li>
        <p>[12/2021] Pass my PhD qualifying exam. Now I am a PhD candidate!</p>
    </li>
    <li>
        <p>[09/2020] Start my PhD journey at HKUST!</p>
    <li>
        <p>[07/2020] Finish my undergraduate study at USTC. Memorable 4 years in Hefei!</p>
    </li>
</div>
</ul>


<h2>Publications</h2> (* indicates co-first authors)
<ul>
	<li>
         <a href='https://2000zrl.github.io'>Improving Continuous Sign Language Recognition with Consistency Constraints and Signer Removal</a> <br>
         <strong><u>Ronglai Zuo</u></strong> and Brian Mak<br>
         Under Review, 2022<br>
    </li>
	<li>
         <a href='https://arxiv.org/abs/2211.01367'>Two-Stream Network for Sign Language Recognition and Translation</a> <br>
         Yutong Chen*, <strong><u>Ronglai Zuo</u></strong>*, Fangyun Wei*, Yu Wu, Shujie Liu, and Brian Mak<br>
         Advances in Neural Information Processing Systems <strong>(NeurIPS)</strong>, New Orleans, USA, 2022, <strong><i>Spotlight</i></strong> <br>
    </li>
	<li>
         <a href='https://openaccess.thecvf.com/content/CVPR2022/html/Zuo_C2SLR_Consistency-Enhanced_Continuous_Sign_Language_Recognition_CVPR_2022_paper.html'>C2SLR: Consistency-enhanced Continuous Sign Language Recognition</a> <br>
         <strong><u>Ronglai Zuo</u></strong> and Brian Mak<br>
         IEEE/CVF Conference on Computer Vision and Pattern Recognition <strong>(CVPR)</strong>, New Orleans, USA, 2022 <br>
    </li>
    <li>
         <a href='https://www.isca-speech.org/archive/interspeech_2022/zuo22_interspeech.html'>Local Context-aware Self-attention for Continuous Sign Language Recognition</a> <br>
         <strong><u>Ronglai Zuo</u></strong> and Brian Mak <br>
         Annual Conference of International Speech Communication Association <strong>(InterSpeech)</strong>, Incheon, Korea, 2022 <br>
    </li>
    <!-- <li>
         <a href='https://arxiv.org/abs/2106.09993'>Accumulative Poisoning Attacks on Real-time Data</a> <br>
         <strong><u>Tianyu Pang</u>*</strong>, Xiao Yang*, Yinpeng Dong, Hang Su, Jun Zhu<br>
         Annual Conference on Neural Information Processing Systems <strong>(NeurIPS)</strong>, Online, 2021 <br>
         <a href="https://github.com/ShawnXYang/AccumulativeAttack">[code]</a>
         <a href="http://ml.cs.tsinghua.edu.cn/~tianyu/accumulative/accumulative.pdf">[slide]</a>
         <a href="https://arxiv.org/abs/2106.09993">[appendix]</a>
    </li> -->
</ul>



<!-- <h2>Preprints and under review</h2>
<ul>
    <li>
         <a href='https://arxiv.org/abs/2105.14785'>Adversarial Training with Rectified Rejection</a> <br>
         <strong><u>Tianyu Pang</u></strong>, Huishuai Zhang, Di He, Yinpeng Dong, Hang Su, Wei Chen, Jun Zhu, Tie-Yan Liu<br>
         arXiv preprint 2105.14785<br>
         <a href="https://github.com/P2333/Rectified-Rejection">[code]</a>
         <a href="https://arxiv.org/abs/2105.14785">[appendix]</a>
    </li>
    <li>
         <a href='https://arxiv.org/abs/2107.01809'>Boosting Transferability of Targeted Adversarial Examples via Hierarchical Generative Networks</a> <br>
         Xiao Yang, Yinpeng Dong, <strong><u>Tianyu Pang</u></strong>, Hang Su, Jun Zhu<br>
         arXiv preprint 2107.01809<br>
    </li>       
</ul> -->


<h2>Honors &amp; Awards</h2>
<ul>
    <li>
        <strong>Outstanding Graduate of USTC</strong>, 2020
    </li>
    <li>
        <strong>Bronze Award for Outstanding Students of USTC</strong>, 2019, 2018, 2017
    </li>
</ul>


<h2>Services</h2>
<ul>
    <li>
        Conference Reviewer: <strong>CVPR</strong> 2023
    </li>
    <li>
        Journal Reviewer: <strong>TMM</strong>
    </li>
</ul>


<h2>Teaching</h2>
<ul>
    <li>
        TA in <strong>COMP2011 Programming with C++</strong>, Spring 2021, Fall 2021
    </li>
</ul>


<div id="footer">
	<div id="footer-text"></div>
</div>
&copy 2022 Ronglai Zuo. Last updated in 11/2022.
</body></html>
